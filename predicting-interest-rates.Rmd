# Predicting Interest Rates
```{r Packages, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) # used for everything
library(data.table) # helpful with big table manipulations
library(quantmod)
library(zoo)
library(PerformanceAnalytics) 
library(corrplot) # makes pretty correlation plots in ggplot
library(fredr)
library(lubridate) # wrangling data
library(glmnet) # regressions
library(caret) #regressions
library(elasticnet) #combination of regressions
library(reshape2)
library(plotly)
library(xgboost)    # used for building XGBoost model
library(e1071)      # used for skewness
library(cowplot)    # used for combining multiple plots 
library(ggcorrplot)
library(RColorBrewer) #PCA Viz
library(idbr) #for population data (census)
```


## Introduction

Both Blinda and I had a fascination with economics data and were particularly interested in finding ways to apply the techniques we have learned in data mining to the field of economics. In many cases, "economics" as an entire field seems way too daunting to attack as a whole. However, in some sense, that seems like a perfect reason to pick it for a data mining project! We could download a massive amount of rather tangentially related data, and find patterns or facts that we find interesting. In the end, our goal was to hopefully build some sort of algorithm or model that backs up the intuition that we have created learning Economics while at Columbia. Of course much of economics comes down to really cool new techniques for finding casual inferences like difference-in-difference and regression discontinuities, but we think some really interesting conclusions should be available with other tools. 

As such our audience for this paper and topics that will be covered in the coming pages should be completely accessible to undergraduates and anyone that has even a very basic understanding of economics terms. While some of the features in the data may not be clear on their own, if they have significant or interesting results they will be explained. For example a term like M1 money. This is just economists making up a fancy term for immediately available money: money that is in checking accounts pretty much and available through debit cards or ATMS. 

## Datasets!

- FRED
- DBnomics
- Census




## Wrangle the data:



## Dataset

```{r API, include=FALSE}
api_key <- "0dc8fa2f5f938907a960e5fd5c20910d"

fredr_set_key(api_key)


idb_api_key("ff03f4c2d4f1a380a4e05b83b2515e75f900bf00")

```


First let's see what the most popular sets of data are!

```{r find top 5 values}
popular_funds_series <- fredr_series_search_text(
    search_text = "federal funds",
    order_by = "popularity",
    sort_order = "desc",
    limit = 5
)

popular_funds_series_id <- popular_funds_series$id

popular_funds_series_id

wanted_funds_series <- c("DGS10", "T10YIE", "CPIAUCSL", "GOLDAMGBD228NLBM")
```

```{r Import Data, message=FALSE, warning=FALSE, include=FALSE}
consumer_price_index_urban <- fredr(
  series_id = "CPIAUCSL"
) %>% 
  rename("consumer_price_index_urban" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date)) 


inflation_expectation <- fredr(
  series_id = "MICH"
) %>% 
  rename("inflation_expectation" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(consumer_price_index_urban, inflation_expectation, by = "dates") %>% 
  select(-date.x, -date.y) 

ten_year_treasury <- fredr(
  series_id = "DGS10"
) %>% 
  rename("ten_year_treasury" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, ten_year_treasury, by = "dates", all = TRUE) %>% 
  select(-`date`)


ten_minus_two_treasury <- fredr(
  series_id = "T10Y2Y"
) %>% 
  rename("ten_minus_two_treasury" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, ten_minus_two_treasury, by = "dates", all = TRUE)%>% 
  select(-date) 





ten_minus_three_months_treasury <- fredr(
  series_id = "T10Y3M"
) %>% 
  rename("ten_minus_three_months_treasury" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, ten_minus_three_months_treasury, by = "dates", all = TRUE)%>% 
  select(-date) 



unemployment_rate <- fredr(
  series_id = "UNRATE"
) %>% 
  rename("unemployment_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, unemployment_rate, by = "dates", all = TRUE)%>% 
  select(-date) 




real_gdp <- fredr(
  series_id = "GDPC1"
) %>% 
  rename("real_gdp" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, real_gdp, by = "dates", all = TRUE)%>% 
  select(-date) 



eff_fed_funds_rate <- fredr(
  series_id = "FEDFUNDS"
) %>% 
  rename("eff_fed_funds_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, eff_fed_funds_rate, by = "dates", all = TRUE)%>% 
  select(-date) 



gdp <- fredr(
  series_id = "GDP"
) %>% 
  rename("gdp" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, gdp, by = "dates", all = TRUE)%>% 
  select(-date) 



thirty_year_fixed_mortgage <- fredr(
  series_id = "MORTGAGE30US"
) %>% 
  rename("thirty_year_fixed_mortgage" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, thirty_year_fixed_mortgage, by = "dates", all = TRUE)%>% 
  select(-date) 


velocity_of_mtwo <- fredr(
  series_id = "M2V"
) %>% 
  rename("velocity_of_mtwo" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, velocity_of_mtwo, by = "dates", all = TRUE)%>% 
  select(-date) 


m_one <- fredr(
  series_id = "M1SL"
) %>% 
  rename("m_one" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, m_one, by = "dates", all = TRUE)%>% 
  select(-date) 



m_two <- fredr(
  series_id = "M2SL"
) %>% 
  rename("m_two" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, m_two, by = "dates", all = TRUE)%>% 
  select(-date) 


all_employees_minus_farmers <- fredr(
  series_id = "PAYEMS"
) %>% 
  rename("all_employees_minus_farmers" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, all_employees_minus_farmers, by = "dates", all = TRUE)%>% 
  select(-date) 


personal_savings_rate <- fredr(
  series_id = "PSAVERT"
) %>% 
  rename("personal_savings_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, personal_savings_rate, by = "dates", all = TRUE)%>% 
  select(-date) 



aaa_corporate_bond_yield <-  fredr(
  series_id = "AAA"
) %>% 
  rename("aaa_corporate_bond_yield" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, aaa_corporate_bond_yield, by = "dates", all = TRUE)%>% 
  select(-date) 


personal_consumption_expenditures <- fredr(
  series_id = "PCE"
) %>% 
  rename("personal_consumption_expenditures" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, personal_consumption_expenditures, by = "dates", all = TRUE)%>% 
  select(-date) 


industrial_production_index <- fredr(
  series_id = "INDPRO"
) %>% 
  rename("industrial_production_index" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, industrial_production_index, by = "dates", all = TRUE)%>% 
  select(-date) 


federal_debt_total <- fredr(
  series_id = "GFDEBTN"
) %>% 
  rename("federal_debt_total" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, federal_debt_total, by = "dates", all = TRUE)%>% 
  select(-date) 


labor_force_participation_rate <- fredr(
  series_id = "CIVPART"
) %>% 
  rename("labor_force_participation_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, labor_force_participation_rate, by = "dates", all = TRUE)%>% 
  select(-date) 


consumer_price_index_nationwide <- fredr(
  series_id = "CPALTT01USM657N"
) %>% 
  rename("consumer_price_index_nationwide" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, consumer_price_index_nationwide, by = "dates", all = TRUE)%>% 
  select(-date) 



s_and_p <- fredr(
  series_id = "CSUSHPINSA"
) %>% 
  rename("s_and_p" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, s_and_p, by = "dates", all = TRUE)%>% 
  select(-date) 



m_three <- fredr(
  series_id = "MABMM301USM189S"
) %>% 
  rename("m_three" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, m_three, by = "dates", all = TRUE)%>% 
  select(-date) 


federal_surplus_or_deficit <-  fredr(
  series_id = "FYFSD"
) %>% 
  rename("federal_surplus_or_deficit" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))%>% 
  mutate(dates = as.numeric(date))

df <- merge(df, federal_surplus_or_deficit, by = "dates", all = TRUE)%>% 
  select(-date) 



house_price_index_nationwide <- fredr(
  series_id = "USSTHPI"
) %>% 
  rename("house_price_index_nationwide" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))%>% 
  mutate(dates = as.numeric(date))

df <- merge(df, house_price_index_nationwide, by = "dates", all = TRUE)%>% 
  select(-date) 


total_vehicle_sales <- fredr(
  series_id = "TOTALSA"
) %>% 
  rename("total_vehicle_sales" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))%>% 
  mutate(dates = as.numeric(date))

df <- merge(df, total_vehicle_sales, by = "dates", all = TRUE)%>% 
  select(-date) 



federal_debt_percent_of_gdp <-  fredr(
  series_id = "GFDEGDQ188S"
) %>% 
  rename("federal_debt_percent_of_gdp" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, federal_debt_percent_of_gdp, by = "dates", all = TRUE)%>% 
  select(-date) 


```

```{r}
df$dates <- as.Date(df$dates) 

df <- as_tibble(df)

dim(df)

```

# Data Summarization

The data as a whole is a beautiful combination of the most downloaded data sources and a variety of different gauges of the market and the United States as a whole.

We initially began by graphing a variety of sources to see how they compare. At first, I thought something was wrong with our data. Below is a graph of Federal Surplus or Deficient (FYFSD) and the US real gross domestic product (GDP). No matter what we graphed against our deficit, everything else appeared to be flat. In reality, this just goes to show the shear size of the deficit the United States is running. 

```{r}
map_dfr(c("FYFSD", "GDPC1"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
    
```

When everything else is compared to each other, they seem more more reasonable. For example, this shows the Unemployment rate and the federal funds rate. 


```{r}
map_dfr(c("UNRATE", "FEDFUNDS"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
```

Fun visualization to compare GDP to GDPC1, or Real GDP. Real GDP is inflation adjusted GDP, meaning it shows the production output when we account for money slowly losing value. 

```{r message=FALSE, warning=FALSE}
map_dfr(c("GDP", "GDPC1"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")


```

Some of the results are interesting to see if they match our intuition. For example, personal savings rate and vehicle sales! 
```{r}
map_dfr(c("TOTALSA", "PSAVERT"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
```

It seems much further off than I expected. Vehicle sales seem almost constant (in teal) with the exception of recessions, before immediately coming back. Personal savings however have seen a steady decrease regardless, although the stimulus appears to have had an amazing effect!

```{r}
modern_df <- filter(df, dates >= "1980-01-01")
modern_df

```

## Problems !
One of the immediate problems with the data was the somewhat random in when it was gathered. Some daily, some monthly, and some seemingly randomly. Others were done by quarter (once every three months), but of course they aren't always released on the first of the month and because we have merged the data daily, it makes it really hard to run any algorithms on. So instead, we shall create a few very clean data frames that are averages. So we created two data frames that were grouped by month and year, averaging the values and ignoring missing values. 

```{r}

df_monthly <- df %>%
  group_by(month = floor_date(dates, "month")) %>%
  summarise(
    consumer_price_index_urban = mean(consumer_price_index_urban, na.rm = TRUE),
    inflation_expectation = mean(inflation_expectation, na.rm = TRUE),
    ten_year_treasury = mean(ten_year_treasury, na.rm = TRUE),
    ten_minus_two_treasury = mean(ten_minus_two_treasury, na.rm = TRUE),
    ten_minus_three_months_treasury = mean(ten_minus_three_months_treasury, na.rm = TRUE),
    unemployment_rate = mean(unemployment_rate, na.rm = TRUE),
    real_gdp = mean(real_gdp, na.rm = TRUE),
    eff_fed_funds_rate = mean(eff_fed_funds_rate, na.rm = TRUE),
    gdp = mean(gdp, na.rm = TRUE),
    thirty_year_fixed_mortgage = mean(thirty_year_fixed_mortgage, na.rm = TRUE),
    m_two = mean(m_two, na.rm = TRUE),
    velocity_of_mtwo = mean(velocity_of_mtwo, na.rm = TRUE),
    m_one = mean(m_one, na.rm = TRUE),
    all_employees_minus_farmers = mean(all_employees_minus_farmers, na.rm = TRUE),
    aaa_corporate_bond_yield = mean(aaa_corporate_bond_yield, na.rm = TRUE),
    personal_savings_rate = mean(personal_savings_rate, na.rm = TRUE),
    personal_consumption_expenditures = mean(personal_consumption_expenditures, na.rm = TRUE),
    industrial_production_index = mean(industrial_production_index, na.rm = TRUE),
    federal_debt_total = mean(federal_debt_total, na.rm = TRUE),
    labor_force_participation_rate = mean(labor_force_participation_rate, na.rm = TRUE),
    consumer_price_index_nationwide = mean(consumer_price_index_nationwide, na.rm = TRUE),
    s_and_p = mean(s_and_p, na.rm = TRUE),
    m_three = mean(m_three, na.rm = TRUE),
    federal_surplus_or_deficit = mean(federal_surplus_or_deficit, na.rm = TRUE),
    house_price_index_nationwide = mean(house_price_index_nationwide, na.rm = TRUE),
    total_vehicle_sales = mean(total_vehicle_sales, na.rm = TRUE),
    federal_debt_percent_of_gdp = mean(federal_debt_percent_of_gdp, na.rm = TRUE)
)



```

```{r}
df_yearly <- df %>%
  group_by(year = floor_date(dates, "year")) %>%
  summarise(
    consumer_price_index_urban = mean(consumer_price_index_urban, na.rm = TRUE),
    inflation_expectation = mean(inflation_expectation, na.rm = TRUE),
    ten_year_treasury = mean(ten_year_treasury, na.rm = TRUE),
    ten_minus_two_treasury = mean(ten_minus_two_treasury, na.rm = TRUE),
    ten_minus_three_months_treasury = mean(ten_minus_three_months_treasury, na.rm = TRUE),
    unemployment_rate = mean(unemployment_rate, na.rm = TRUE),
    real_gdp = mean(real_gdp, na.rm = TRUE),
    eff_fed_funds_rate = mean(eff_fed_funds_rate, na.rm = TRUE),
    gdp = mean(gdp, na.rm = TRUE),
    thirty_year_fixed_mortgage = mean(thirty_year_fixed_mortgage, na.rm = TRUE),
    m_two = mean(m_two, na.rm = TRUE),
    velocity_of_mtwo = mean(velocity_of_mtwo, na.rm = TRUE),
    m_one = mean(m_one, na.rm = TRUE),
    all_employees_minus_farmers = mean(all_employees_minus_farmers, na.rm = TRUE),
    aaa_corporate_bond_yield = mean(aaa_corporate_bond_yield, na.rm = TRUE),
    personal_savings_rate = mean(personal_savings_rate, na.rm = TRUE),
    personal_consumption_expenditures = mean(personal_consumption_expenditures, na.rm = TRUE),
    industrial_production_index = mean(industrial_production_index, na.rm = TRUE),
    federal_debt_total = mean(federal_debt_total, na.rm = TRUE),
    labor_force_participation_rate = mean(labor_force_participation_rate, na.rm = TRUE),
    consumer_price_index_nationwide = mean(consumer_price_index_nationwide, na.rm = TRUE),
    s_and_p = mean(s_and_p, na.rm = TRUE),
    m_three = mean(m_three, na.rm = TRUE),
    federal_surplus_or_deficit = mean(federal_surplus_or_deficit, na.rm = TRUE),
    house_price_index_nationwide = mean(house_price_index_nationwide, na.rm = TRUE),
    total_vehicle_sales = mean(total_vehicle_sales, na.rm = TRUE),
    federal_debt_percent_of_gdp = mean(federal_debt_percent_of_gdp, na.rm = TRUE)
)

df_yearly_cleaned <- na.omit(df_yearly)

df_yearly_cleaned
```

In fact, to show a point of how inconcistent some of the data was, simply cleaning the data by pulling only months where all data series had at least an entry yields absolutely no months. So instead, for the monthly approach, we decided to take a different approach. Instead, we started eliminating poor data features, ones that didn't line up enough with the majority of the data we had. 

# Explore

```{r}
df_monthly_modern <- df_monthly %>% 
  filter(month >= "1987-01-01")

monthly_nas <- df_monthly_modern %>% 
  summarise_all(funs(sum(is.na(.))))

monthly_nas
```

This yielded an interesting finding. In fact, there appear to be two types of data here. Data collected quarterly and data collected monthly (or even more granularly). Then there are exceptions like total federal debt which is collected once each year in september. So in this way, the data was split into 2 groups. 

```{r}
data_monthly <- df_monthly %>% 
  select(-real_gdp, -gdp, -velocity_of_mtwo, -federal_debt_total, -federal_surplus_or_deficit, -house_price_index_nationwide, -house_price_index_nationwide, -federal_debt_percent_of_gdp) %>% 
  na.omit()
  
data_monthly
```


```{r}
data_quarterly <- df_monthly %>% 
  select(-federal_debt_total, - federal_debt_percent_of_gdp, -federal_surplus_or_deficit) %>% 
  na.omit()

data_quarterly
```

For intuition, I decided to run regression on the yearly dataset, as it has a few additional features that we lose when picking quarterly or monthly, so I thought it couldn't hurt to try. As for what to predict, I thought the most interesting would actually be the inflation expectation. This is a metrix calculated by Michigan State which says what do American's think the interest rate is going to do in the immediate future. In a sense, this allows us to approximate what the effects of all the other features are on Americans. 

```{r}
linear_model_yearly <- lm(data = df_yearly_cleaned, inflation_expectation ~ .)

summary(linear_model_yearly)
```

```{r}
plot(df_yearly_cleaned[1:10, 1:10])
```



Unsurprisingly, there is not a ton of significance throughout, and there is likely high colinearity between many of these. However, it is interesting to look at what was pulled through so far. Of the features that had a significance code, total vehicle sales and M2 money supply seem to be the most impactful. M2 money is a count of all money including cash and checking deposits (M1) as well as money saved in savings accounts, money market securities, mutual funds and other time deposits. In a sense, it is a good gauge of how much physical money Americans have that they are able to spend. 


```{r}
df_yearly_minus_date <- df_yearly_cleaned %>% select(-year)

correlation_df <- round(cor(df_yearly_minus_date),2)

correlation_df_melt <- melt(correlation_df)
gz <- ggplot(correlation_df_melt, mapping = aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(text = element_text(size = 8)) + 
  ggtitle("Heat map for correlation") + 
  ylab("")+
  xlab("")+
  scale_fill_distiller(palette = "RdPu")
ggplotly(gz, tooltip = "text")

```
From an even very brief glance, we can see quite a few of our features of very correlated. In a sense, this should be expected in a finance market with everything working together. For example, the Effective federal funds rate is designed to regulate many of the other features we have in our dataset, so it seems reasonable they are correlated. Also, some features are derivatives of others such as federal deficit and federal debt. 



```{r}
findCorrelation(
  cor(df_yearly_minus_date),
  cutoff = 0.99,
  verbose = FALSE,
  names = FALSE
)

ggcorrplot(correlation_df, hc.order = TRUE, type = "lower")

```
Although the text is not readable (for now), the correlation is better shown when only highly correlated values are left in. 

Of course, now the immediate next step is likely to try Ridge, Lasso and Elasticnet. These should "fix", at least to some extent the problem of multicolinearity. 


## Working with monthly data
To get a more granular approach which is more appropriate, the other techniques will be tried on the monthly data instead of the yearly. 

## Train and Test

Note: It may be worth while to bootstrap or upsample here. 

The data will be split into train and test datasets. The model will be trained on the train data and evaluated on the test data.

```{r}

data_monthly_dateless <- data_monthly %>% 
  select(-month)

set.seed(100) 


index = sample(1:nrow(data_monthly_dateless), 0.7*nrow(data_monthly_dateless)) 

train = data_monthly_dateless[index,] # Create the training data 
test = data_monthly_dateless[-index,] # Create the test data
dim(train)
dim(test)
```


```{r}
cols <-  colnames(data_monthly_dateless) 

pre_proc_val <- preProcess(train[,cols], method = c("center", "scale"))
train[,cols] = predict(pre_proc_val, train[,cols])
test[,cols] = predict(pre_proc_val, test[,cols])


```


Scaled Linear Model
```{r}
linear_scaled <- lm(inflation_expectation ~ . , data = train)
summary(linear_scaled)
```

These are more exciting results! Using the scaled monthly data, we have some very significant results with non-trivial estimates. Things that we think effect our inflation expectation: how the cost of a basket of goods changes nationwide, the ten year treasury note, the unemployment rate, the 30 fixed mortgage rate, number of employees in the US. As would be expected, the bonds, the unemployment rate and the number of employees are all inversely correlated. We would assume prices won't be forced up when there are still people that want work, or when the bond yields are really high. 

As for the linear model itself, it is not terrific but also not as poor of a model as one might expect. Our $R^2$ values are above .65 and our RMSE is within reason. 

```{r}

#Step 1 - create the evaluation metrics function

eval_metrics = function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 2))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 2))
    print(c("Adjusted R^2", adj_r2)) #Adjusted R-squared
    print(c("RMSE", as.character(round(sqrt(sum(resids2)/N), 2)))) #RMSE
}

predictions = predict(linear_scaled, newdata = train)
eval_metrics(linear_scaled, train, predictions, target = 'inflation_expectation')

# Step 3 - predicting and evaluating the model on test data
predictions = predict(linear_scaled, newdata = test)
eval_metrics(linear_scaled, test, predictions, target = 'inflation_expectation')
```

$R^2$ remains the same and RMSE increases modestly on test data. Still not wonderful but better than I would naturally expect!


```{r}
# Regularization
cols_reg = cols

dummies <- dummyVars(inflation_expectation ~ ., data = data_monthly_dateless[,cols_reg])

train_dummies = predict(dummies, newdata = train[,cols_reg])

test_dummies = predict(dummies, newdata = test[,cols_reg])

print(dim(train_dummies)); print(dim(test_dummies))
```

Initally, because of the extreme correlation in some, I would expect lasso to be the technique we eventually want to resort to but I figure we might as well try ridge as well. 

```{r}

# ridge

x = as.matrix(train_dummies)
y_train = train$inflation_expectation

x_test = as.matrix(test_dummies)
y_test = test$inflation_expectation

lambdas <- 10^seq(4, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, lambda = lambdas)

cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda

plot(ridge_reg, main = "Ridge Regression")


```

Optimal lambda comes out at 0.001. I'm skeptical of this value immediately, and it may require more exploration to see if there are optimizations to improve it. 

```{r}
#lasso
lambdas <- 10^seq(4, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best

lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best

plot(lasso_reg, main = "Lasso Regression")


mean(lasso_reg$lambda.1se)

```

Taking a step back to look at what we are estimating. The expected values are between 0 and 6 effectively, for what will be the inflation in the immediate future. Really where our modeling might be letting us down is in the fact that the majority of the time, you can guess 2.5 to 3.5 % inflation and be right. 


```{r}
#elastic net

histogram(data_monthly_dateless$inflation_expectation, breaks = 20)
```


```{r}
cv_5 = trainControl(method = "cv", number = 5)

inflation_elnet <- train(
  inflation_expectation ~.,data = data_monthly_dateless, 
  method = "glmnet", 
  trControl = cv_5
)

inflation_elnet
```

```{r}
#similar results

inflation_elnet_best <- train(
  inflation_expectation ~ . ^ 2, data = data_monthly_dateless,
  method = "glmnet",
  trControl = cv_5,
  tuneLength = 10
)

get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(inflation_elnet_best)
```
We end up with very similar results: because alpha is at 1 so it is practicly doing the exact same algorithm as above. 


## Principal Component Analysis

```{r}
obs_x <- na.omit(data_monthly) %>% 
  rename(date = month)

ord <- order(obs_x$date)
obs_x <- obs_x[ord, ]
dates <- as.Date(obs_x$date)
obs_x$date <- NULL
obs_x <- sapply(obs_x, diff)

pr_out <- prcomp(obs_x, center=TRUE, scale=TRUE)

```

```{r}
plot(cumsum(pr_out$sdev^2)/ sum(pr_out$sdev^2))
abline(h = 0.95)

#same number of rows as x, with only two most efficient columns
W <-  pr_out$x[,1:2]

plot(pr_out$sdev, main="")
```

Appears to be a noticable drop off at 5 actually.




```{r}
# cor matrix

# correlation <-round(cor(df_test,use = "complete.obs"),2)
# correlation[upper.tri(correlation)] <- 0
# correlation
# 
# # print it out
# # library(xtable)
# # print(xtable(upper), type="html")
# 
# 
# 
# 
# # stages of the economic cycle (....)
# 
# 
# # k means
# # relatiom btw FEDFUNDS, Gold, CPI; cover any confonder
# 
# df_km <- cbind(DGS10_chg, FEDFUNDS_chg,CPIAUCS_chg,GOLDAMGBD228NLBM_chg)
# 
# df_km <- na.omit(df_km)
# 
# km_out <- kmeans(df_km, centers = 5)  #goodness of the classification k-means
# 
# output <- list()
# for(i in 1:6){
#         output[[i]] <- kmeans(coredata(df_km), i)
# }
# output  # maybe k =5
# 
# 
# # number of observation
# km_out <- kmeans(df_km, centers = 5)
# km_out$size
# km_out$centers


```


## Engineer features based on population
Here we introduce population data from the census in order to hopefully look at per capita figures. This should hopefully find some new realizations. The US Census API actually has data for the entire world which is fascinating! So maybe we


```{r}
lex <- get_idb(
  country = "all",
  year = 2021,
  variables = c("name", "e0"),
  geometry = TRUE
)

ggplot(lex, aes(fill = e0)) + 
  theme_bw() + 
  geom_sf() + 
  coord_sf(crs = 'ESRI:54030') + 
  scale_fill_viridis_c() + 
  labs(fill = "Life expectancy \nat birth (2021)")
```

```{r}
population <- get_idb(
  country = "USA",
  year = 1978:2021,
  age = 0:100,
  sex = c("male", "female")
) %>% 
  group_by(year) %>% 
  summarise(population = sum(pop)) 

ggplot(data = population, mapping = aes(x = year, y = population)) + 
  geom_point() + 
  geom_line() + 
  scale_y_continuous("Populations") +
  scale_x_continuous()+
  labs(title = "Population of the USA", subtitle = "Nothing very surprising here")
  
```

Out of caution, this will only be applied to the yearly data as this is the most granular level we have. In the future, you could linearly estimate the data assuming an equal distribution of birthdays throughout the year and that should be close. 
```{r}
population$year <- as.Date(ISOdate(population$year, 1, 1))

data_per_capita <- merge(df_yearly_cleaned, population) %>% 
  mutate(car_sales_percap = total_vehicle_sales / population) %>% 
  mutate(house_price_percap = house_price_index_nationwide / population) %>% 
  mutate(s_and_p_percap = s_and_p / population) %>% 
  mutate(m_two_percap = m_two / population)
```
The reason to create this type of feature is actually a much more helpful way of standardizing some of these values. For example, car sales will naturally climb with more people. But if it climbs per capita that would be a good indicator that inflationary levels and expecations. House prices could be affected in a similar way. 

From our initial regression, we know that m2 money was important. However, this could be inappropriate because there should be more money with a lot more people so this is helpful to make sure that truly is significant. 




## Conclusion

### Value

Saved time? Decreased uncertainty? What did we find?










Checklist
[] algorithm to explore the relationship between different features in the data
- graphical summary
- quantitative evaluation
- engineer a feature

[] Veryify results
- subset?
- How robust is this?
- Uncertainty? How does this affect the conclusions

[] Discussion of data dredging/snooping

[] github readme





