# Predicting Interest Rates
```{r Packages, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) # used for everything
library(data.table) # helpful with big table manipulations
library(quantmod)
library(zoo)
library(PerformanceAnalytics) 
library(corrplot) # makes pretty correlation plots in ggplot
library(fredr)
library(lubridate) # wrangling data
library(glmnet) # regressions
library(caret) #regressions
library(elasticnet) #combination of regressions
library(reshape2)
library(plotly)
library(xgboost)    # used for building XGBoost model
library(e1071)      # used for skewness
library(cowplot)    # used for combining multiple plots 
library(ggcorrplot)
library(RColorBrewer) #PCA Viz
library(idbr) #for population data (census)
require(reshape2) #plotting all columns
library(gridExtra)
```

```{r API, include=FALSE}
api_key <- "0dc8fa2f5f938907a960e5fd5c20910d"

fredr_set_key(api_key)


idb_api_key("ff03f4c2d4f1a380a4e05b83b2515e75f900bf00")

```

## Introduction

Both Blinda and I had a fascination with economics data and were particularly interested in finding ways to apply the techniques we have learned in data mining to the field of economics. In many cases, "economics" as an entire field seems way too daunting to attack as a whole. However, in some sense, that seems like a perfect reason to pick it for a data mining project! We could download a massive amount of rather tangentially related data, and find patterns or facts that we find interesting. In the end, our goal was to hopefully build some sort of algorithm or model that backs up the intuition that we have created learning Economics while at Columbia. Of course much of economics comes down to really cool new techniques for finding casual inferences like difference-in-difference and regression discontinuities, but we think some really interesting conclusions should be available with other tools. 

As such our audience for this paper and topics that will be covered in the coming pages should be completely accessible to undergraduates and anyone that has even a very basic understanding of economics terms. While some of the features in the data may not be clear on their own, if they have significant or interesting results they will be explained. For example a term like M1 money. This is just economists making up a fancy term for immediately available money: money that is in checking accounts pretty much and available through debit cards or ATMS. 

## Datasets!

Because we didn't have a particular focus to our economic exploration, we decided that we would try and gather as wide of a range of data as we could, combine them into one large dataset and then look for interesting paths to follow.

This strategy led us to "FRED"; not a person but rather the Federal Reserve Bank of St. Louis which provides an incredible amount of data direct from one of the 12 U.S. federal reserve banks. As a result, this data is the most direct we can get to the Economy at a sense and is commonly used by economic researchers. Also, thanks to their API, our research project should be able to be updated simply by rerunning the markdown file to include any new data. 

The beauty of using FRED data is that there is not just one dataset, but hundreds of different ones. As such, not knowing exactly what we wanted, we found we could actually use the api function in order to find the most popular datasets by calls! This allowed us to rely on the experience of other researchers to see what data was popular and what we should be looking at. 

Here is an example api to FRED searching through the series and asking for the 5 most popular data series. 

```{r}
popular_funds_series <- fredr_series_search_text(
    search_text = "federal funds",
    order_by = "popularity",
    sort_order = "desc",
    limit = 5
)
```


```{r find top 5 values, include=FALSE}
popular_funds_series_id <- popular_funds_series$id

popular_funds_series_id
```

As a result, we ended up calling 27 different datasets and combining them together. These datasets range widely in what they describe and wildly on their date ranges. As examples, the datasets describe everything from a consumer price index, estimating the average price of goods nationwide, to the number of vehicle sales. Larger macroeconomics data was pulled as well including gross domestic product (GDP), unemployment rate, or even the prices of bonds. In summary, we have from prior knowledge classified the data into five categories: 

1. Asset price: 
- intent and finding 
- house_price_index_nationwide
- s_and_p
- total_vehicle_sales


2. Debt supply and demand: 
- federal_debt_percent_of_gdp
- federal_surplus_or_deficit

3. economic expectation/confidence:
- consumer_price_index_nationwide
- consumer_price_index_urban
- personal_savings_rate
- personal_consumption_expenditures
- gdp
- real_gdp
- real_gdp_per_capita
- all_employees_minus_farm
- unemployment_rate
- industrial_production_index
- labor_force_participation_rate
- inflation_expectation

4. secondary market 
- ten_minus_three_months_treasury
- ten_year_treasury
- aaa_corporate_bond_yield
- ten_minus_two_treasury
- thirty_year_fixed_mortgage

5. Federal Reserve (Fed decided)
- m_one
- m_two
- m_three
- velocity_of_mtwo
- eff_fed_funds_rate


```{r Import Data, message=FALSE, warning=FALSE, include=FALSE, cache = TRUE}
consumer_price_index_urban <- fredr(
  series_id = "CPIAUCSL"
) %>% 
  rename("consumer_price_index_urban" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date)) 


inflation_expectation <- fredr(
  series_id = "MICH"
) %>% 
  rename("inflation_expectation" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(consumer_price_index_urban, inflation_expectation, by = "dates") %>% 
  select(-date.x, -date.y) 

ten_year_treasury <- fredr(
  series_id = "DGS10"
) %>% 
  rename("ten_year_treasury" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, ten_year_treasury, by = "dates", all = TRUE) %>% 
  select(-`date`)


ten_minus_two_treasury <- fredr(
  series_id = "T10Y2Y"
) %>% 
  rename("ten_minus_two_treasury" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, ten_minus_two_treasury, by = "dates", all = TRUE)%>% 
  select(-date) 

ten_minus_three_months_treasury <- fredr(
  series_id = "T10Y3M"
) %>% 
  rename("ten_minus_three_months_treasury" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, ten_minus_three_months_treasury, by = "dates", all = TRUE)%>% 
  select(-date) 



unemployment_rate <- fredr(
  series_id = "UNRATE"
) %>% 
  rename("unemployment_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, unemployment_rate, by = "dates", all = TRUE)%>% 
  select(-date) 




real_gdp <- fredr(
  series_id = "GDPC1"
) %>% 
  rename("real_gdp" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, real_gdp, by = "dates", all = TRUE)%>% 
  select(-date) 



eff_fed_funds_rate <- fredr(
  series_id = "FEDFUNDS"
) %>% 
  rename("eff_fed_funds_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, eff_fed_funds_rate, by = "dates", all = TRUE)%>% 
  select(-date) 



gdp <- fredr(
  series_id = "GDP"
) %>% 
  rename("gdp" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, gdp, by = "dates", all = TRUE)%>% 
  select(-date) 



thirty_year_fixed_mortgage <- fredr(
  series_id = "MORTGAGE30US"
) %>% 
  rename("thirty_year_fixed_mortgage" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, thirty_year_fixed_mortgage, by = "dates", all = TRUE)%>% 
  select(-date) 


velocity_of_mtwo <- fredr(
  series_id = "M2V"
) %>% 
  rename("velocity_of_mtwo" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, velocity_of_mtwo, by = "dates", all = TRUE)%>% 
  select(-date) 


m_one <- fredr(
  series_id = "M1SL"
) %>% 
  rename("m_one" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, m_one, by = "dates", all = TRUE)%>% 
  select(-date) 



m_two <- fredr(
  series_id = "M2SL"
) %>% 
  rename("m_two" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, m_two, by = "dates", all = TRUE)%>% 
  select(-date) 


all_employees_minus_farmers <- fredr(
  series_id = "PAYEMS"
) %>% 
  rename("all_employees_minus_farmers" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, all_employees_minus_farmers, by = "dates", all = TRUE)%>% 
  select(-date) 


personal_savings_rate <- fredr(
  series_id = "PSAVERT"
) %>% 
  rename("personal_savings_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, personal_savings_rate, by = "dates", all = TRUE)%>% 
  select(-date) 



aaa_corporate_bond_yield <-  fredr(
  series_id = "AAA"
) %>% 
  rename("aaa_corporate_bond_yield" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, aaa_corporate_bond_yield, by = "dates", all = TRUE)%>% 
  select(-date) 


personal_consumption_expenditures <- fredr(
  series_id = "PCE"
) %>% 
  rename("personal_consumption_expenditures" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, personal_consumption_expenditures, by = "dates", all = TRUE)%>% 
  select(-date) 


industrial_production_index <- fredr(
  series_id = "INDPRO"
) %>% 
  rename("industrial_production_index" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, industrial_production_index, by = "dates", all = TRUE)%>% 
  select(-date) 


federal_debt_total <- fredr(
  series_id = "GFDEBTN"
) %>% 
  rename("federal_debt_total" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, federal_debt_total, by = "dates", all = TRUE)%>% 
  select(-date) 


labor_force_participation_rate <- fredr(
  series_id = "CIVPART"
) %>% 
  rename("labor_force_participation_rate" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, labor_force_participation_rate, by = "dates", all = TRUE)%>% 
  select(-date) 


consumer_price_index_nationwide <- fredr(
  series_id = "CPALTT01USM657N"
) %>% 
  rename("consumer_price_index_nationwide" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, consumer_price_index_nationwide, by = "dates", all = TRUE)%>% 
  select(-date) 



s_and_p <- fredr(
  series_id = "CSUSHPINSA"
) %>% 
  rename("s_and_p" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, s_and_p, by = "dates", all = TRUE)%>% 
  select(-date) 



m_three <- fredr(
  series_id = "MABMM301USM189S"
) %>% 
  rename("m_three" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, m_three, by = "dates", all = TRUE)%>% 
  select(-date) 


federal_surplus_or_deficit <-  fredr(
  series_id = "FYFSD"
) %>% 
  rename("federal_surplus_or_deficit" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))%>% 
  mutate(dates = as.numeric(date))

df <- merge(df, federal_surplus_or_deficit, by = "dates", all = TRUE)%>% 
  select(-date) 



house_price_index_nationwide <- fredr(
  series_id = "USSTHPI"
) %>% 
  rename("house_price_index_nationwide" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))%>% 
  mutate(dates = as.numeric(date))

df <- merge(df, house_price_index_nationwide, by = "dates", all = TRUE)%>% 
  select(-date) 


total_vehicle_sales <- fredr(
  series_id = "TOTALSA"
) %>% 
  rename("total_vehicle_sales" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))%>% 
  mutate(dates = as.numeric(date))

df <- merge(df, total_vehicle_sales, by = "dates", all = TRUE)%>% 
  select(-date) 



federal_debt_percent_of_gdp <-  fredr(
  series_id = "GFDEGDQ188S"
) %>% 
  rename("federal_debt_percent_of_gdp" = value) %>% 
  select(-realtime_start, -realtime_end, - series_id) %>% 
  mutate(dates = as.numeric(date))

df <- merge(df, federal_debt_percent_of_gdp, by = "dates", all = TRUE)%>% 
  select(-date) 


```


```{r include=FALSE}
df$dates <- as.Date(df$dates) 

df <- as_tibble(df)

dim(df)

```

## Data wrangling 

Because the data comes from the same source, we assumed that they would merge together nicely; however, this was not the case. The primary problem was when the data was collected. and for most datasets this was different. In general, data was collected either: daily, weekly, monthly, quarterly or yearly. However, this means that not every dataframe had the same dates to merge on, so the final data frame had a large amount of NAs. 

Additionally, not all data was collected starting in the same timeframe. For example, federal debt has existed for longer than the inception of some of the other metrics. As a result, some debt data goes all the way back to January of 1901 (through the API at least), whereas other datasets start as recently as 1987.

However in the end, through the use of `merge `, we were able to build one complete dataset out of 27 different sources and with more than 16,000 rows.


## Data Exploration

The data as a whole is a beautiful combination of the most downloaded data sources and a variety of different gauges of the market and the United States as a whole.

We initially began by graphing a variety of sources to see how they compare. At first, I thought something was wrong with our data. Below is a graph of Federal Surplus or Deficit (FYFSD) and the US real gross domestic product (GDPC1). 

```{r echo=FALSE}
map_dfr(c("FYFSD", "GDPC1"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
```

No matter what we graphed against the US deficit, everything else appeared to be flat. In reality, this just goes to show the shear size of the deficit the United States is running. In fact, 3 trillion next to almost any other number in our data set is massive. This may be important to remember later, and make sure this dataset isn't having undue influence on our models. 

When everything else is compared to each other, they seem more more reasonable. For example, this shows the unemployment rate (UNRATE) and the federal funds rate (FEDFUNDS). 

```{r echo=FALSE}
map_dfr(c("UNRATE", "FEDFUNDS"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
```

Data like this actually appears really interesting and is telling the closer you look at it. At first, it may appear just like noise when in fact there really is a relationship at play. The federal funds rate appears to spike, immediately followed by the unemployment rate. In a sense, the unemployment rate is trailing the federal funds rate. 

In reality, its not quite as simple. More often when economic times are good, the federal funds rate is raising, so that it can be lowered when the unemployment rate rises. It is a tool for economists to try and help the economy. And after a while, another recession uccurs which forces the rate back down to drive inflation. 

In terms of data manipulation, this graph could be slightly difficult for something like regression to parse. In a sense, the data is incredibly time dependent being time series data and as a result, may not fit well in basic regression especially. 

Other graphs showed other very intertwined relationships. For example, there is not much closer than comparing GDP to GDPC1, or Real GDP. Real GDP is inflation adjusted GDP, meaning it shows the production output when we account for money slowly losing value. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
map_dfr(c("GDP", "GDPC1"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
```

Some of the results are interesting to see if they match our intuition. For example, personal savings rate and vehicle sales! 

```{r echo=FALSE}
map_dfr(c("TOTALSA", "PSAVERT"), fredr) %>%
  ggplot(data = ., mapping = aes(x = date, y = value, color = series_id)) +
    geom_line() +
    labs(x = "Observation Date", y = "Rate", color = "Series")
```

It seems much further off than I expected. Vehicle sales seem almost constant (in teal) with the exception of recessions, before immediately coming back. Personal savings however have seen a steady decrease regardless, although the stimulus appears to have had an amazing effect!


## More problems with dates !

One of the immediate problems with the data was the somewhat random in when it was gathered. Some daily, some monthly, and some seemingly randomly. Others were done by quarter (once every three months), but of course they aren't always released on the first of the month and because we have merged the data daily, it makes it really hard to run any algorithms on. 

As an example, the federal debt appears to be gathered just once a year in September, whereas other data is gathered every single day. So the data needed to be processed in such as way that we could more easily work with it. Our conclusive way of doing so was to create summarized data frames. So we created two data frames that were grouped by month and year, averaging the values and including NaN for all values that had no data points.  


```{r include=FALSE}

df_monthly <- df %>%
  group_by(month = floor_date(dates, "month")) %>%
  summarise(
    consumer_price_index_urban = mean(consumer_price_index_urban, na.rm = TRUE),
    inflation_expectation = mean(inflation_expectation, na.rm = TRUE),
    ten_year_treasury = mean(ten_year_treasury, na.rm = TRUE),
    ten_minus_two_treasury = mean(ten_minus_two_treasury, na.rm = TRUE),
    ten_minus_three_months_treasury = mean(ten_minus_three_months_treasury, na.rm = TRUE),
    unemployment_rate = mean(unemployment_rate, na.rm = TRUE),
    real_gdp = mean(real_gdp, na.rm = TRUE),
    eff_fed_funds_rate = mean(eff_fed_funds_rate, na.rm = TRUE),
    gdp = mean(gdp, na.rm = TRUE),
    thirty_year_fixed_mortgage = mean(thirty_year_fixed_mortgage, na.rm = TRUE),
    m_two = mean(m_two, na.rm = TRUE),
    velocity_of_mtwo = mean(velocity_of_mtwo, na.rm = TRUE),
    m_one = mean(m_one, na.rm = TRUE),
    all_employees_minus_farmers = mean(all_employees_minus_farmers, na.rm = TRUE),
    aaa_corporate_bond_yield = mean(aaa_corporate_bond_yield, na.rm = TRUE),
    personal_savings_rate = mean(personal_savings_rate, na.rm = TRUE),
    personal_consumption_expenditures = mean(personal_consumption_expenditures, na.rm = TRUE),
    industrial_production_index = mean(industrial_production_index, na.rm = TRUE),
    federal_debt_total = mean(federal_debt_total, na.rm = TRUE),
    labor_force_participation_rate = mean(labor_force_participation_rate, na.rm = TRUE),
    consumer_price_index_nationwide = mean(consumer_price_index_nationwide, na.rm = TRUE),
    s_and_p = mean(s_and_p, na.rm = TRUE),
    m_three = mean(m_three, na.rm = TRUE),
    federal_surplus_or_deficit = mean(federal_surplus_or_deficit, na.rm = TRUE),
    house_price_index_nationwide = mean(house_price_index_nationwide, na.rm = TRUE),
    total_vehicle_sales = mean(total_vehicle_sales, na.rm = TRUE),
    federal_debt_percent_of_gdp = mean(federal_debt_percent_of_gdp, na.rm = TRUE)
)

```

```{r include=FALSE}
df_yearly <- df %>%
  group_by(year = floor_date(dates, "year")) %>%
  summarise(
    consumer_price_index_urban = mean(consumer_price_index_urban, na.rm = TRUE),
    inflation_expectation = mean(inflation_expectation, na.rm = TRUE),
    ten_year_treasury = mean(ten_year_treasury, na.rm = TRUE),
    ten_minus_two_treasury = mean(ten_minus_two_treasury, na.rm = TRUE),
    ten_minus_three_months_treasury = mean(ten_minus_three_months_treasury, na.rm = TRUE),
    unemployment_rate = mean(unemployment_rate, na.rm = TRUE),
    real_gdp = mean(real_gdp, na.rm = TRUE),
    eff_fed_funds_rate = mean(eff_fed_funds_rate, na.rm = TRUE),
    gdp = mean(gdp, na.rm = TRUE),
    thirty_year_fixed_mortgage = mean(thirty_year_fixed_mortgage, na.rm = TRUE),
    m_two = mean(m_two, na.rm = TRUE),
    velocity_of_mtwo = mean(velocity_of_mtwo, na.rm = TRUE),
    m_one = mean(m_one, na.rm = TRUE),
    all_employees_minus_farmers = mean(all_employees_minus_farmers, na.rm = TRUE),
    aaa_corporate_bond_yield = mean(aaa_corporate_bond_yield, na.rm = TRUE),
    personal_savings_rate = mean(personal_savings_rate, na.rm = TRUE),
    personal_consumption_expenditures = mean(personal_consumption_expenditures, na.rm = TRUE),
    industrial_production_index = mean(industrial_production_index, na.rm = TRUE),
    federal_debt_total = mean(federal_debt_total, na.rm = TRUE),
    labor_force_participation_rate = mean(labor_force_participation_rate, na.rm = TRUE),
    consumer_price_index_nationwide = mean(consumer_price_index_nationwide, na.rm = TRUE),
    s_and_p = mean(s_and_p, na.rm = TRUE),
    m_three = mean(m_three, na.rm = TRUE),
    federal_surplus_or_deficit = mean(federal_surplus_or_deficit, na.rm = TRUE),
    house_price_index_nationwide = mean(house_price_index_nationwide, na.rm = TRUE),
    total_vehicle_sales = mean(total_vehicle_sales, na.rm = TRUE),
    federal_debt_percent_of_gdp = mean(federal_debt_percent_of_gdp, na.rm = TRUE)
)

df_yearly_cleaned <- na.omit(df_yearly)
```




As a result, we had a yearly data table that consisted of 34 years, ranging 1987 to 2021, and a monthly dataframe ranging the same years but with a more granular, 1246 monthly data points.

To show a point of how inconcistent some of the data was, simply cleaning the monthly data by pulling only months where all data series had at least an entry yields absolutely no months. So instead, for the monthly approach, we decided to take a different approach. To figure that out, we graphed the number of missing values by series in the monthly data. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
df_monthly_modern <- df_monthly %>% 
  filter(month >= "1987-01-01")

monthly_nas <- df_monthly_modern %>% 
  summarise_all(funs(sum(is.na(.)))) 

data_long <- gather(monthly_nas, month, consumer_price_index_urban,inflation_expectation, ten_year_treasury, ten_minus_two_treasury, ten_minus_three_months_treasury, unemployment_rate, real_gdp, eff_fed_funds_rate, gdp, thirty_year_fixed_mortgage, m_two, velocity_of_mtwo, m_one, all_employees_minus_farmers, aaa_corporate_bond_yield, personal_savings_rate, personal_consumption_expenditures, industrial_production_index, federal_debt_total, labor_force_participation_rate, consumer_price_index_nationwide, s_and_p, m_three, federal_surplus_or_deficit, house_price_index_nationwide, total_vehicle_sales, federal_debt_percent_of_gdp )


data_nas <- data_long %>% 
  rename(Missing_Values = consumer_price_index_urban) %>%
  rename(dataseries = month) %>% 
  ggplot(data = ., aes(x = dataseries, y = Missing_Values)) + 
    geom_bar(stat = "identity") + 
    coord_flip() + 
    ggtitle("Number of missing months by series") 

data_nas

```

This yielded an interesting finding. In fact, there appear to be two types of data here. Data collected quarterly and data collected monthly (or even more granularly). Then there are exceptions like total federal debt which is collected once each year in september. Data sets needed to be segregated by their collection frequency; ones that didn't line up enough with the majority of the data we moved to their own group. This left us with monthly, weekly and daily data in one table knon as monthly, whereas the data that was collected quarterly was put into another data frame.  

```{r include=FALSE}
data_monthly <- df_monthly %>% 
  select(-real_gdp, -gdp, -velocity_of_mtwo, -federal_debt_total, -federal_surplus_or_deficit, -house_price_index_nationwide, -house_price_index_nationwide, -federal_debt_percent_of_gdp) %>% 
  na.omit()
  

data_quarterly <- df_monthly %>% 
  select(-federal_debt_total, - federal_debt_percent_of_gdp, -federal_surplus_or_deficit) %>% 
  na.omit()

```

## Initial modeling tests

For intuition, I decided to run regression on the yearly dataset, as it has a few additional features that we lose when picking quarterly or monthly, so I thought it couldn't hurt to try. As for what to predict, I thought the most interesting would actually be the inflation expectation. This is a metric calculated by Michigan State which says what do American's think the interest rate is going to do in the immediate future. In a sense, this allows us to approximate what the effects of all the other features are on Americans. 

This is exciting because it acts as a proxy for how people feel about the economy, which we find to be a much more interesting question than what will bond prices be next year. In a sense, it allows us to analyze how humans feel about the economy: anxious or encouraged. 

```{r echo=FALSE}
linear_model_yearly <- lm(data = df_yearly_cleaned, inflation_expectation ~ .)

summary(linear_model_yearly)
```

Unsurprisingly, there is not a ton of significance throughout, and there is likely high colinearity between many of these. However, it is interesting to look at what was pulled through so far. Of the features that had a significance code, total vehicle sales and M2 money supply seem to be the most impactful. M2 money is a count of all money including cash and checking deposits (M1) as well as money saved in savings accounts, money market securities, mutual funds and other time deposits. In a sense, it is a good gauge of how much physical money Americans have that they are able to spend. 

However, there are quite a few shortcomings to this initial approach. Noramlization might be helpful, and much of the data is highly correlated. For example, high correlation can very easily be identified by graphing:

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(df_yearly_cleaned[1:10, 1:10])

df_yearly_minus_date <- df_yearly_cleaned %>% select(-year)

correlation_df <- round(cor(df_yearly_minus_date),2)

correlation_df_melt <- melt(correlation_df)
gz <- ggplot(correlation_df_melt, mapping = aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(text = element_text(size = 8)) + 
  ggtitle("Heat map for correlation") + 
  ylab("")+
  xlab("")+
  scale_fill_distiller(palette = "RdPu")
ggplotly(gz, tooltip = "text")

```


From an even very brief glance, we can see quite a few of our features of very correlated. In a sense, this should be expected in a finance market with everything working together. For example, the Effective federal funds rate is designed to regulate many of the other features we have in our dataset, so it seems reasonable they are correlated. Also, some features are derivatives of others such as federal deficit and federal debt. 

In fact, there are 7 features that have a correlation of over .99 with at lest one other feature. 

```{r echo=FALSE}
findCorrelation(
  cor(df_yearly_minus_date),
  cutoff = 0.99,
  verbose = FALSE,
  names = TRUE
)
```

We can get a partial view of some of these strong relationships through a further graph exploration of a subset of the data. 

```{r echo=FALSE}

ggcorrplot(correlation_df[10:21, 10:21], hc.order = TRUE, type = "lower")

```


Of course, now the immediate next step is likely to try Ridge, Lasso and Elasticnet. These should "fix", at least to some extent the problem of multicolinearity. 

## Working with monthly data
To get a more granular approach which is more appropriate, the other techniques will be tried on the monthly data instead of the yearly. 

### Train and Test

The data will be split into train and test datasets. The model will be trained on the train data and evaluated on the test data. This helps us gauge our robustness and see if we are at risk of overfitting. An inherent problem when dealing with financial data is the ability to say that your model is predictive of the future. It is commonplace for any stock or financial advisor to say that past results are not indicative of future porformance. As a result, just because there was a relationship between variables that we can see, doesn't mean that will remain true. This is increasingly true if there is a new type of "shock" to the economy, like a global pandemic. 

```{r echo=FALSE}
data_monthly_dateless <- data_monthly %>% 
  select(-month)

set.seed(100) 

index = sample(1:nrow(data_monthly_dateless), 0.7*nrow(data_monthly_dateless)) 

train = data_monthly_dateless[index,] # Create the training data 
test = data_monthly_dateless[-index,] # Create the test data

cols <-  colnames(data_monthly_dateless) 

pre_proc_val <- preProcess(train[,cols], method = c("center", "scale"))
train[,cols] = predict(pre_proc_val, train[,cols])
test[,cols] = predict(pre_proc_val, test[,cols])
```


### Scaled Linear Model
Our first new approach is to normalize the data in a hope of reducing some of the effects that certain datasets could have when their scales were so much larger than others. 

```{r echo=FALSE}
linear_scaled <- lm(inflation_expectation ~ . , data = train)
summary(linear_scaled)
```

These are more exciting results! Using the scaled monthly data, we have some very significant results with non-trivial estimates. Things that we think effect our inflation expectation: how the cost of a basket of goods changes nationwide, the ten year treasury note, the unemployment rate, the 30 fixed mortgage rate, number of employees in the US. As would be expected, the bonds, the unemployment rate and the number of employees are all inversely correlated. We would assume prices won't be forced up when there are still people that want work, or when the bond yields are really high. 

As for the linear model itself, it is not terrific but also not as poor of a model as one might expect. Our $R^2$ values are above .65 and our RMSE is within reason. 

Running on train data:

```{r echo=FALSE}

#Step 1 - create the evaluation metrics function

eval_metrics = function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 2))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 2))
    print(c("Adjusted R^2", adj_r2)) #Adjusted R-squared
    print(c("RMSE", as.character(round(sqrt(sum(resids2)/N), 2)))) #RMSE
}

predictions = predict(linear_scaled, newdata = train)
eval_metrics(linear_scaled, train, predictions, target = 'inflation_expectation')


```
Rerunning on test data:

```{r echo=FALSE}
# Step 3 - predicting and evaluating the model on test data
predictions = predict(linear_scaled, newdata = test)
eval_metrics(linear_scaled, test, predictions, target = 'inflation_expectation')
```



$R^2$ remains the same and RMSE increases modestly on test data. Still not wonderful but better than I would naturally expect!


```{r echo=FALSE}
# Regularization
cols_reg = cols

dummies <- dummyVars(inflation_expectation ~ ., data = data_monthly_dateless[,cols_reg])

train_dummies = predict(dummies, newdata = train[,cols_reg])

test_dummies = predict(dummies, newdata = test[,cols_reg])

print(dim(train_dummies)); print(dim(test_dummies))
```

Initally, because of the extreme correlation in some, I would expect lasso to be the technique we eventually want to resort to but I figure we might as well try ridge as well. 

```{r echo=FALSE}
# ridge

x = as.matrix(train_dummies)
y_train = train$inflation_expectation

x_test = as.matrix(test_dummies)
y_test = test$inflation_expectation

lambdas <- 10^seq(4, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, lambda = lambdas)

cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda

plot(ridge_reg, main = "Ridge Regression")


```

Optimal lambda comes out at 0.001. I'm skeptical of this value immediately, and it may require more exploration to see if there are optimizations to improve it. 

```{r echo=FALSE}
#lasso
lambdas <- 10^seq(4, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best

lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best

plot(lasso_reg, main = "Lasso Regression")


mean(lasso_reg$lambda.1se)

```

Taking a step back to look at what we are estimating. The expected values are between 0 and 6 effectively, for what will be the inflation in the immediate future. Really where our modeling might be letting us down is in the fact that the majority of the time, you can guess 2.5 to 3.5 % inflation and be right. 


```{r echo=FALSE}
#elastic net

histogram(data_monthly_dateless$inflation_expectation, breaks = 20)
```


```{r echo=FALSE}
cv_5 = trainControl(method = "cv", number = 5)

inflation_elnet <- train(
  inflation_expectation ~.,data = data_monthly_dateless, 
  method = "glmnet", 
  trControl = cv_5
)

inflation_elnet
```

```{r echo=FALSE}
#similar results

inflation_elnet_best <- train(
  inflation_expectation ~ . ^ 2, data = data_monthly_dateless,
  method = "glmnet",
  trControl = cv_5,
  tuneLength = 10
)

get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(inflation_elnet_best)
```
We end up with very similar results: because alpha is at 1 so it is practicly doing the exact same algorithm as above. 


## Principal Component Analysis

```{r}
obs_x <- na.omit(data_monthly) %>% 
  rename(date = month)

ord <- order(obs_x$date)
obs_x <- obs_x[ord, ]
dates <- as.Date(obs_x$date)
obs_x$date <- NULL
obs_x <- sapply(obs_x, diff)

pr_out <- prcomp(obs_x, center=TRUE, scale=TRUE)
```




```{r echo=FALSE}
plot(cumsum(pr_out$sdev^2)/ sum(pr_out$sdev^2))
abline(h = 0.95)

#same number of rows as x, with only two most efficient columns
W <-  pr_out$x[,1:2]

plot(pr_out$sdev, main="")
```

Appears to be a noticable drop off at 5 actually.

## Economic Cycles

At first, clustering didn't make much sense for such a wide variety of data. After all, what ways could we describe the entire economy of the United States? 

Well, when looking at the data graphed on FRED's website, it is clear that there is one major factor that the government is looking for: are we in a recession. In fact, it could be possible to pull out the different parts of the economic cycle from clustering. There should be characteristic markings in the data when the economy is in a recession versus when it is booming. The question is are those markings clear. 

```{r echo=FALSE}
km_out <- kmeans(data_monthly_dateless, centers = 3)

#table(km_out$cluster)
```
At first the distrubtion of groups actually looked decent. Using three phases, growth, recession and a sort of stagnation, we were hoping to capture the phases. However, running it on the original datasets led to three, completely chronological groups. Factors like debt, gdp and deficit likely were too much for the klustering to overcome. 


```{r}
data_monthly_scaled <- scale(data_monthly_dateless, scale = TRUE)

km_out <- kmeans(data_monthly_dateless, centers = 3)

#table(km_out$cluster)

#km_out$cluster
```
Even after scaling the data, chronological groups were all that klustering could pull out. 




## Engineer features based on population
Here we introduce population data from the census in order to hopefully look at per capita figures. This should hopefully find some new realizations. The US Census API actually has data for the entire world which is fascinating! 

One reason why klustering might not be working so well is the effects that population growth is having on the country. There is undoubtably differences between the last few decades and klustering is pulling that out through the numbers. 


```{r}
lex <- get_idb(
  country = "all",
  year = 2021,
  variables = c("name", "e0"),
  geometry = TRUE
)

ggplot(lex, aes(fill = e0)) + 
  theme_bw() + 
  geom_sf() + 
  coord_sf(crs = 'ESRI:54030') + 
  scale_fill_viridis_c() + 
  labs(fill = "Life expectancy \nat birth (2021)")
```

```{r}
population <- get_idb(
  country = "USA",
  year = 1978:2021,
  age = 0:100,
  sex = c("male", "female")
) %>% 
  group_by(year) %>% 
  summarise(population = sum(pop)) 

ggplot(data = population, mapping = aes(x = year, y = population)) + 
  geom_point() + 
  geom_line() + 
  scale_y_continuous("Populations") +
  scale_x_continuous()+
  labs(title = "Population of the USA", subtitle = "Nothing very surprising here")
  
```

Out of caution, this will only be applied to the yearly data as this is the most granular level we have. In the future, you could linearly estimate the data assuming an equal distribution of birthdays throughout the year and that should be close. 

The reason to create this type of feature is actually a much more helpful way of standardizing some of these values. For example, car sales will naturally climb with more people. But if it climbs per capita that would be a good indicator that inflationary levels and expecations. House prices could be affected in a similar way. 

From our initial regression, we know that m2 money was important. However, this could be inappropriate because there should be more money with a lot more people so this is helpful to make sure that truly is significant. 


```{r}
population$year <- as.Date(ISOdate(population$year, 1, 1))


colnames(df_yearly_cleaned)

data_per_capita <- merge(df_yearly_cleaned, population) %>% 
  mutate(car_sales_percap = total_vehicle_sales / population) %>% 
  mutate(house_price_percap = house_price_index_nationwide / population) %>% 
  mutate(s_and_p_percap = s_and_p / population) %>% 
  mutate(m_two_percap = m_two / population) %>% 
  mutate(consumer_price_index_percap = consumer_price_index_urban / population) %>% 
  mutate(aaa_bond_percap = aaa_corporate_bond_yield / population) %>% 
  mutate(all_empoyees_percap = all_employees_minus_farmers / population) %>% 
  select(year, inflation_expectation, car_sales_percap,house_price_percap,s_and_p_percap,s_and_p_percap,m_two_percap,consumer_price_index_percap,aaa_bond_percap,all_empoyees_percap, personal_consumption_expenditures, eff_fed_funds_rate)


```

```{r echo=FALSE}
percap_s_and_p_plot <- ggplot(data = data_per_capita, mapping = aes(x = year, y = s_and_p_percap)) + 
  geom_line()  



s_and_p_plot <- ggplot(data = df_yearly_cleaned, mapping = aes(x = year, y = s_and_p)) +
  geom_line()



grid.arrange(percap_s_and_p_plot, s_and_p_plot)
```

Unfortunately, there doesn't appear to be much of a difference in the S and P to say the last. Let's see how regression ends up. 

```{r}
data_percap_dateless <- data_per_capita %>% 
  select(-year)

scaled_percap <- as_tibble(scale(data_percap_dateless))

linear_percap <- lm(inflation_expectation ~ ., data = scaled_percap)

summary(linear_percap)
```
```{r}
km_out <- kmeans(scaled_percap, centers = 3)
```

However, even scaled for population, clustering fails to recognize anything other than time, and the simplified per-capita regression provided no additional information compared to the full model. 



## Conclusion

### Value

Saved time? Decreased uncertainty? What did we find?










Checklist
- quantitative evaluation
- engineer a feature

[] Veryify results
- subset?
- How robust is this?
- Uncertainty? How does this affect the conclusions

[] Discussion of data dredging/snooping

[] github readme





